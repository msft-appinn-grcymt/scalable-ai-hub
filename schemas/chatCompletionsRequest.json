{
    "required": [
        "messages"
    ],
    "type": "object",
    "properties": {
        "messages": {
            "minItems": 1,
            "type": "array",
            "items": {
                "required": [
                    "role",
                    "content"
                ],
                "type": "object",
                "properties": {
                    "role": {
                        "enum": [
                            "system",
                            "user",
                            "assistant"
                        ],
                        "type": "string",
                        "description": "The role of the author of this message."
                    },
                    "content": {
                        "type": "string",
                        "description": "The contents of the message"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the user in a multi-user chat"
                    }
                }
            },
            "description": "The messages to generate chat completions for, in the chat format."
        },
        "temperature": {
            "maximum": 2,
            "minimum": 0,
            "type": "number",
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.",
            "default": 1,
            "nullable": true,
            "example": 1
        },
        "top_p": {
            "maximum": 1,
            "minimum": 0,
            "type": "number",
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or `temperature` but not both.",
            "default": 1,
            "nullable": true,
            "example": 1
        },
        "n": {
            "maximum": 128,
            "minimum": 1,
            "type": "integer",
            "description": "How many chat completion choices to generate for each input message.",
            "default": 1,
            "nullable": true,
            "example": 1
        },
        "stream": {
            "type": "boolean",
            "description": "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.",
            "default": false,
            "nullable": true
        },
        "stop": {
            "oneOf": [
                {
                    "type": "string",
                    "nullable": true
                },
                {
                    "maxItems": 4,
                    "minItems": 1,
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Array minimum size of 1 and maximum of 4"
                }
            ],
            "description": "Up to 4 sequences where the API will stop generating further tokens.",
            "default": null
        },
        "max_tokens": {
            "type": "integer",
            "description": "The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).",
            "default": "inf"
        },
        "presence_penalty": {
            "maximum": 2,
            "minimum": -2,
            "type": "number",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "default": 0
        },
        "frequency_penalty": {
            "maximum": 2,
            "minimum": -2,
            "type": "number",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "default": 0
        },
        "logit_bias": {
            "type": "object",
            "description": "Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.",
            "nullable": true
        },
        "user": {
            "type": "string",
            "description": "A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.",
            "example": "user-1234"
        }
    },
    "x-apim-inline": true
}